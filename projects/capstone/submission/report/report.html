<!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
        <style>
/*--------------------------------------------------------------------------------------------- * Copyright (c) Microsoft Corporation. All rights reserved. * Licensed under the MIT License. See License.txt in the project root for license information. *--------------------------------------------------------------------------------------------*/ body { font-family: "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback"; font-size: 14px; padding: 0 12px; line-height: 22px; word-wrap: break-word; } body.scrollBeyondLastLine { margin-bottom: calc(100vh - 22px); } body.showEditorSelection .code-line { position: relative; } body.showEditorSelection .code-active-line:before, body.showEditorSelection .code-line:hover:before { content: ""; display: block; position: absolute; top: 0; left: -12px; height: 100%; } body.showEditorSelection li.code-active-line:before, body.showEditorSelection li.code-line:hover:before { left: -30px; } .vscode-light.showEditorSelection .code-active-line:before { border-left: 3px solid rgba(0, 0, 0, 0.15); } .vscode-light.showEditorSelection .code-line:hover:before { border-left: 3px solid rgba(0, 0, 0, 0.40); } .vscode-dark.showEditorSelection .code-active-line:before { border-left: 3px solid rgba(255, 255, 255, 0.4); } .vscode-dark.showEditorSelection .code-line:hover:before { border-left: 3px solid rgba(255, 255, 255, 0.60); } .vscode-high-contrast.showEditorSelection .code-active-line:before { border-left: 3px solid rgba(255, 160, 0, 0.7); } .vscode-high-contrast.showEditorSelection .code-line:hover:before { border-left: 3px solid rgba(255, 160, 0, 1); } img { max-width: 100%; max-height: 100%; } a { color: #4080D0; text-decoration: none; } a:focus, input:focus, select:focus, textarea:focus { outline: 1px solid -webkit-focus-ring-color; outline-offset: -1px; } hr { border: 0; height: 2px; border-bottom: 2px solid; } h1 { padding-bottom: 0.3em; line-height: 1.2; border-bottom-width: 1px; border-bottom-style: solid; } h1, h2, h3 { font-weight: normal; } h1 code, h2 code, h3 code, h4 code, h5 code, h6 code { font-size: inherit; line-height: auto; } a:hover { color: #4080D0; text-decoration: underline; } table { border-collapse: collapse; } table > thead > tr > th { text-align: left; border-bottom: 1px solid; } table > thead > tr > th, table > thead > tr > td, table > tbody > tr > th, table > tbody > tr > td { padding: 5px 10px; } table > tbody > tr + tr > td { border-top: 1px solid; } blockquote { margin: 0 7px 0 5px; padding: 0 16px 0 10px; border-left: 5px solid; } code { font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback"; font-size: 14px; line-height: 19px; } body.wordWrap pre { white-space: pre-wrap; } .mac code { font-size: 12px; line-height: 18px; } code > div { padding: 16px; border-radius: 3px; overflow: auto; } /** Theming */ .vscode-light { color: rgb(30, 30, 30); } .vscode-dark { color: #DDD; } .vscode-high-contrast { color: white; } .vscode-light code { color: #A31515; } .vscode-dark code { color: #D7BA7D; } .vscode-light code > div { background-color: rgba(220, 220, 220, 0.4); } .vscode-dark code > div { background-color: rgba(10, 10, 10, 0.4); } .vscode-high-contrast code > div { background-color: rgb(0, 0, 0); } .vscode-high-contrast h1 { border-color: rgb(0, 0, 0); } .vscode-light table > thead > tr > th { border-color: rgba(0, 0, 0, 0.69); } .vscode-dark table > thead > tr > th { border-color: rgba(255, 255, 255, 0.69); } .vscode-light h1, .vscode-light hr, .vscode-light table > tbody > tr + tr > td { border-color: rgba(0, 0, 0, 0.18); } .vscode-dark h1, .vscode-dark hr, .vscode-dark table > tbody > tr + tr > td { border-color: rgba(255, 255, 255, 0.18); } .vscode-light blockquote, .vscode-dark blockquote { background: rgba(127, 127, 127, 0.1); border-color: rgba(0, 122, 204, 0.5); } .vscode-high-contrast blockquote { background: transparent; border-color: #fff; }
</style>
<style>
/* Tomorrow Theme */ /* http://jmblog.github.com/color-themes-for-google-code-highlightjs */ /* Original theme - https://github.com/chriskempson/tomorrow-theme */ /* Tomorrow Comment */ .hljs-comment, .hljs-quote { color: #8e908c; } /* Tomorrow Red */ .hljs-variable, .hljs-template-variable, .hljs-tag, .hljs-name, .hljs-selector-id, .hljs-selector-class, .hljs-regexp, .hljs-deletion { color: #c82829; } /* Tomorrow Orange */ .hljs-number, .hljs-built_in, .hljs-builtin-name, .hljs-literal, .hljs-type, .hljs-params, .hljs-meta, .hljs-link { color: #f5871f; } /* Tomorrow Yellow */ .hljs-attribute { color: #eab700; } /* Tomorrow Green */ .hljs-string, .hljs-symbol, .hljs-bullet, .hljs-addition { color: #718c00; } /* Tomorrow Blue */ .hljs-title, .hljs-section { color: #4271ae; } /* Tomorrow Purple */ .hljs-keyword, .hljs-selector-tag { color: #8959a8; } .hljs { display: block; overflow-x: auto; color: #4d4d4c; padding: 0.5em; } .hljs-emphasis { font-style: italic; } .hljs-strong { font-weight: bold; }
</style>
<style>
ul.contains-task-list { padding-left: 0; } ul ul.contains-task-list { padding-left: 40px; } .task-list-item { list-style-type: none; } .task-list-item-checkbox { vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'HelveticaNeue-Light', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
    </head>
    <body>
        <style>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<h1 id="machine-learning-engineer-nanodegree">Machine Learning Engineer Nanodegree</h1>
<h2 id="capstone-report">Capstone Report</h2>
<p>Rodrigo Moreira Fagundes
December 23rd, 2018</p>
<h2 id="introduction">Introduction</h2>
<p>In such a large developing country as Brazil, with so many different economic and cultural setups, tackling modern slavery efficiently is a huge challenge. According the Observatório Digital do Trabalho Escravo (https://observatorioescravo.mpt.mp.br/), from 2003 to 2018, 44,229 people were rescued from degrading working conditions in 3,318 inspections (13.33 rescues per inspaction). 2,006 successful diligences took place in 766 of the 5,570 brazilian municipalities (a coverage of 13.75%). If we add the 1,847 inspections with no rescues, the coverage raises to a 37.9%, with 2,112 locations, but it reveals a success rate of 52.06%. Since 2015, the number of diligences have benn falling, returning to the yearly inspections frequency seen in 2003-2007.</p>
<p>Some regions are not accessible, taking too long to mobilize an inspection - allowing perpetrators to move their operations or just hide their illegal aspects during the audit. Traditionally, the inspections are mobilized based on a denounce and evidences that support it. The most vulnerable people, though, lack the opportunity to reach government agencies.</p>
<p>In opposition, urban centers with high population density have a large number of enterprises to be verified, rendering a traditional coverage goal unrealistic - both due to cost and manpower. Modern slavery practices in urban centers tend to be disguised as just poor labor standards or practices, facing long disputes before estabilishing the perpetration.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>Given the scenarios, government agencies have to craft a way to concentrate its resources in targets that reach more vulnerable people. One way to create such a prioritization can be trying to identify, using classification, the locations that would most likely to result in more rescues per diligence. Some inputs are very promising for that task, such as municipalities profiles - based on census data provided by IBGE (Instituto Nacional de Geografia e Estatística) - and the record of previous inspections - from MTE (Ministério do Trabalho e Emprego). The output of the classification will indicate municipalities in which inspections may result in maximum rescue rates.</p>
<h3 id="datasets-and-inputs">Datasets and Inputs</h3>
<p>For this study, we're using three datasets. The first is a collection of information on municipality's census, collected by IBGE (Instituto Nacional de Geografia e Estatística), available to the public. The comprehensive dataset, with no relevant missing data so far, is a suitable source for identifying profiles and similarities between locations. Is is also a reliable dataset, once the census conducted by IBGE goes through a rigorous methodology.</p>
<p>The second and third datasets will be the disidentificated registers of operations and inspections. They contain information on the municipalities where inspections took place, how many people were rescued from degrading work conditions, their origin and where they claimed to reside at the moment. In the current study, they'll be used as a base for risk rating, which in turn will become the label for classification.</p>
<h3 id="solution-statement">Solution Statement</h3>
<p>One solution to the problem can be resource optimization by defining high priorities munuicipalities based on statistical inference. By using municipalities similarities and previous diligences data, it's reasonable to focus on locations that are most likely to result in a more effective action, rescuing more people in a single inspection, for instance. By prioritizing municipalities according to the distribution of rescues per inspection, the model can be repeated, hopefully with decreasing numbers of perpetrations.</p>
<h2 id="exploring-the-datasets">Exploring the datasets</h2>
<h3 id="census">Census</h3>
<p>The census data, collected by IBGE (https://www.ibge.gov.br/) was provided by Smartlab (http://smartlab.mpt.mp.br) in December 22nd, 2018 as a CSV file. The dataset is the same used in the Observatório Digital do Trabalho Escravo (http://observatorioescravo.mpt.mp.br). It contains 73 indicators, including GDP, employed population by age, among others. We took the more comprehensive data, from 2010.</p>
<p>In order to remove municipalities' identification, we appended the average of rescues per inspections from the rescues data to the census dataset. The input was generated by MTE (http://mte.gov.br) and provided by Smartlab. It is also used in the Observatório Digital do Trabalho Escravo.</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># Reading the CSV</span>
df = pd.read_csv(<span class="hljs-string">'data/dataset.csv'</span>)

<span class="hljs-comment"># Formating data from vl_indicador</span>
<span class="hljs-keyword">import</span> re
df[<span class="hljs-string">'vl_no_format'</span>] = df[<span class="hljs-string">'vl_indicador'</span>].map(<span class="hljs-keyword">lambda</span> x: float(re.sub(<span class="hljs-string">'[\.]'</span>, <span class="hljs-string">''</span>, str(x))))

<span class="hljs-comment"># Pivoting the data</span>
df = df.pivot_table(index=<span class="hljs-string">'cd_mun_ibge'</span>, columns=<span class="hljs-string">'ds_indicador_curto'</span>, values=<span class="hljs-string">'vl_no_format'</span>).reset_index().drop(columns=[<span class="hljs-string">'cd_mun_ibge'</span>])

<span class="hljs-comment"># Exploring the data</span>
print(df.shape)
df.head()
</div></code></pre>
<pre><code>(5565, 113)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>ds_indicador_curto</th>
      <th>Alfabetização das pessoas de 15 anos ou mais</th>
      <th>Analfabetismo das pessoas de 15 a 24 anos</th>
      <th>Aprendizes em relação à população ocupada</th>
      <th>Area Territorial</th>
      <th>Bolsa Família, PETI ou outros programas sociais - Domicílios que recebem</th>
      <th>Crescimento da população de 2000 a 2010</th>
      <th>Crianças Ocupadas</th>
      <th>Crianças e Adolescentes ocupados como trabalhadores domésticos, sobre o total da população de 10 a 17 anos</th>
      <th>Crianças e Adolescentes ocupados no trabalho doméstico</th>
      <th>Crianças e adolescentes ocupados</th>
      <th>...</th>
      <th>Trabalhadores por conta própria contribuintes de 16 a 64 anos</th>
      <th>Trabalho doméstico no total de ocupados de 10 a 17 anos</th>
      <th>Valor Adicionado Bruto</th>
      <th>Valor Adicionado Bruto a preços correntes</th>
      <th>Valor Adicionado Bruto a preços correntes - Participação</th>
      <th>Valor Adicionado Bruto a preços correntes - Participação Total</th>
      <th>Valor Adicionado Bruto a preços correntes - Total</th>
      <th>Valor Adicionado Bruto, a preços correntes, dos serviços, da Administração</th>
      <th>Índice de GINI do rendimento do trabalho principal - Pessoas de 16 a 64 anos ocupadas com rendimento</th>
      <th>Índice de GINI do rendimento domiciliar dos domicílios particulares</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>420.0</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7.047750e+13</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.544000e+15</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>142882.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>4.681699e+13</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>473.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.573000e+03</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>5066.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7028754.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>89750.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>92.0</td>
      <td>1096192.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>335910.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6049.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 113 columns</p>
</div>
<p>The census data provided don't seem to be as thorough as expected. It should be revisited in the future studies, should the density proves too low to provide a strong profile for the municipality.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Checking sparsity/density</span>
sdf = df.to_sparse()
print(sdf.density)
</div></code></pre>
<pre><code>0.1546629137545818
</code></pre>
<p>Since the dataset revealed itself too sparse, we'll conduct the study using just the rescues dataset. It contains a high dimensionality (higher than the census), because of the depth and granularity of some dimensions.</p>
<h3 id="rescue-data">Rescue data</h3>
<p>The inspection data was colected from the Observatório Digital do Trabalho Escravo (http://observatorioescravo.mpt.mp.br) in December 22nd, 2018 as a CSV file. It contains register from rescues with high dimensionality, since the indicators were built taking into account the munber of responses of each rescuee survey and includes info on gender, race, instruction, age, occupation (current and desired), among others.</p>
<p>All data are numeric indicatiors, with the ammount of people. They can be divided into 3 main segments:</p>
<ul>
<li>'_rgt_' marks the data from rescues or rescuees;</li>
<li>'_res_' reveals information about the people that reside in that place and were rescued anywhere</li>
<li>'_nat_' means the amount of people born in a municipality, regardless of where they were rescued</li>
</ul>
<p>Another important variable is <em>ds_agregacao_primaria</em>, which is a second level of granularity, an specialization of the indicator.</p>
<p><strong>For example:</strong> <em>te_res_raca</em> with ds_agregacao_primaria 'Branca' in the municipality 0000000 is an indicator that counts the number of white people that resided in the city 0000000, regardless of where they were rescued.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Reading the CSV</span>
df = pd.read_csv(<span class="hljs-string">'data/dataset_resgates.csv'</span>)

<span class="hljs-comment"># Setting a default level to summarized indicators</span>
df[<span class="hljs-string">'ds_agreg_primaria'</span>] = df[<span class="hljs-string">'ds_agreg_primaria'</span>].fillna(<span class="hljs-string">'default'</span>)

<span class="hljs-comment"># Pivoting the data</span>
<span class="hljs-comment"># Since the data come from surveys, the absence of information means no occurence of that instance, thus the zero-filling.</span>
df = df.pivot_table(
    index=<span class="hljs-string">'cd_mun_ibge'</span>,
    columns=[<span class="hljs-string">'cd_indicador'</span>,<span class="hljs-string">'ds_agreg_primaria'</span>],
    values=<span class="hljs-string">'vl_indicador'</span>,
    fill_value=<span class="hljs-number">0</span>
).reset_index().drop(columns=[<span class="hljs-string">'cd_mun_ibge'</span>])

<span class="hljs-comment"># Exploring the data</span>
print(df.shape)

sdf = df.to_sparse()
print(sdf.density)

df.head()
</div></code></pre>
<pre><code>(3371, 1518)
1.0
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="7" halign="left">te_nat_cnae</th>
      <th>...</th>
      <th>te_res_raca</th>
      <th colspan="5" halign="left">te_res_raca_idade</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>Abate de suínos, aves e outros pequenos animais</th>
      <th>Administração pública em geral</th>
      <th>Aluguel de máquinas e equipamentos não especificados anteriormente</th>
      <th>Aparelhamento e outros trabalhos em pedras</th>
      <th>Apicultura</th>
      <th>Armazenamento</th>
      <th>Atividade Medica Ambulatorial com Recursos para Realizacao de Procedimentos Cirurgicos</th>
      <th>...</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1518 columns</p>
</div>
<h2 id="data-preparation">Data Preparation</h2>
<p>First of all, municipalities with no rescue will be removed from the dataset. As proposed, only those with actual rescue will be subject of the current study.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Removing rows with te_rgt = 0</span>
df = df[df.te_rgt.default != <span class="hljs-number">0</span>]

print(df.shape)

sdf = df.to_sparse()
print(sdf.density)

df.head()
</div></code></pre>
<pre><code>(753, 1518)
1.0
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="7" halign="left">te_nat_cnae</th>
      <th>...</th>
      <th>te_res_raca</th>
      <th colspan="5" halign="left">te_res_raca_idade</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>Abate de suínos, aves e outros pequenos animais</th>
      <th>Administração pública em geral</th>
      <th>Aluguel de máquinas e equipamentos não especificados anteriormente</th>
      <th>Aparelhamento e outros trabalhos em pedras</th>
      <th>Apicultura</th>
      <th>Armazenamento</th>
      <th>Atividade Medica Ambulatorial com Recursos para Realizacao de Procedimentos Cirurgicos</th>
      <th>...</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>10</td>
      <td>16</td>
      <td>27</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>72</td>
      <td>203</td>
      <td>67.75</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>3</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>16</td>
      <td>10.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>16</td>
      <td>17</td>
      <td>8.50</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>14</td>
      <td>14.00</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1518 columns</p>
</div>
<p>The dataset has a high dimensionality. In order to perform the model, we need to rationalize it. Before the transformation to CSV, we categorized the age of recuees from numeric bins to 'minor' and 'adult' - this is important to differentiate child labor and slavery of adults.</p>
<p>We'll also remove occupational data. Its granularity is too high, leading to a right-tailed distribution, concentrated near the origin.</p>
<pre class="hljs"><code><div>idx = pd.IndexSlice

occupation = df.loc[idx[:], idx[<span class="hljs-string">'te_nat_cnae'</span>, :]]
occupation_sample = occupation.head(<span class="hljs-number">10</span>)
occupation_sample = occupation_sample.loc[:, (occupation_sample != <span class="hljs-number">0</span>).any(axis=<span class="hljs-number">0</span>)]

pd.plotting.scatter_matrix(occupation_sample, alpha = <span class="hljs-number">0.3</span>, figsize = (<span class="hljs-number">14</span>,<span class="hljs-number">8</span>), diagonal = <span class="hljs-string">'kde'</span>);
occupation_sample.head(<span class="hljs-number">10</span>)
</div></code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th colspan="12" halign="left">te_nat_cnae</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>Atividades de Apoio a Producao Florestal</th>
      <th>Comércio varejista de produtos de padaria, laticínio, doces, balas e semelhantes</th>
      <th>Construcao de Rodovias e Ferrovias</th>
      <th>Criacao de Bovinos para Corte</th>
      <th>Cultivo de Arroz</th>
      <th>Desdobramento de madeira</th>
      <th>Fabricacao de Alcool</th>
      <th>Fabricacao de Laticinios</th>
      <th>Ignorado</th>
      <th>Producao de Ferro-Gusa</th>
      <th>Produção florestal - florestas nativas</th>
      <th>Servico de Inseminacao Artificial em Animais</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>13</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<p>Occupations data is relevant in the context of tackling modern slavery. These data can be confronted with the joined information from CAGED (Cadastro Geral de Empregados e Desempregados) and census data, so that people can be receive instruction for jobs that are actually in demand or can be moved to loactions where there are job openings for the rescuee's desired occupation.</p>
<p>To the current study, though, they don't contributeto the outcome (high granularity and near-zero right-tailed distribution). Therefore, all occupations data will be removed.</p>
<pre class="hljs"><code><div>occup_cols = [col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> df.axes[<span class="hljs-number">1</span>].get_level_values(<span class="hljs-number">0</span>) <span class="hljs-keyword">if</span> any([<span class="hljs-keyword">True</span> <span class="hljs-keyword">for</span> part <span class="hljs-keyword">in</span> [<span class="hljs-string">'cnae'</span>,<span class="hljs-string">'ocup'</span>] <span class="hljs-keyword">if</span> part <span class="hljs-keyword">in</span> col])]
occup_cols_no_rep = list(set(occup_cols))

<span class="hljs-comment"># Columns to be removed</span>
print(occup_cols_no_rep)

<span class="hljs-comment"># Dropping occupation columns</span>
df_no_occup = df.drop(occup_cols_no_rep, axis=<span class="hljs-number">1</span>, level=<span class="hljs-number">0</span>)
print(df_no_occup.shape)
df_no_occup.head()
</div></code></pre>
<pre><code>['te_res_ocup_pret', 'te_nat_cnae', 'te_res_cnae', 'te_nat_instrucao_ocup_pret', 'te_res_ocup_atual', 'te_nat_sexo_cnae', 'te_nat_ocup_atual', 'te_nat_ocup_pret']
(753, 69)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="3" halign="left">te_nat_idade</th>
      <th colspan="4" halign="left">te_nat_instrucao</th>
      <th>...</th>
      <th>te_res_raca</th>
      <th colspan="5" halign="left">te_res_raca_idade</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>adult</th>
      <th>default</th>
      <th>minor</th>
      <th>5º Ano Completo</th>
      <th>6º ao 9º Ano Incompl</th>
      <th>&gt;Ignorado</th>
      <th>Analfabeto</th>
      <th>...</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>10</td>
      <td>16</td>
      <td>27</td>
      <td>22</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>11</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>72</td>
      <td>203</td>
      <td>67.75</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>3</td>
      <td>6</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>16</td>
      <td>10.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>16</td>
      <td>17</td>
      <td>8.50</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>14</td>
      <td>14.00</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 69 columns</p>
</div>
<p>After dropping the occupations data, 60 features remained in the dataset.</p>
<p>From that collection, some actually have a secondary aggregation feature, not present in the CSV. Those will be removed.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Getting the remaining indicators' codes</span>
print(df_no_occup.axes[<span class="hljs-number">1</span>].get_level_values(<span class="hljs-number">0</span>))

<span class="hljs-comment"># Indicators known (business logic) to have a secondary level</span>
cols_sec_level = [<span class="hljs-string">'te_nat_instrucao_idade'</span>, <span class="hljs-string">'te_nat_raca_idade'</span>, <span class="hljs-string">'te_res_instrucao_idade'</span>, <span class="hljs-string">'te_res_raca_idade'</span>]
df_final = df_no_occup.drop(cols_sec_level, axis=<span class="hljs-number">1</span>, level=<span class="hljs-number">0</span>)

print(df_final.shape)

<span class="hljs-comment"># Checking sparsity/density</span>
sdf = df.to_sparse()
print(sdf.density)

df_final.head()
</div></code></pre>
<pre><code>Index(['te_insp_rgt', 'te_inspecoes', 'te_nat', 'te_nat_idade', 'te_nat_idade',
       'te_nat_idade', 'te_nat_instrucao', 'te_nat_instrucao',
       'te_nat_instrucao', 'te_nat_instrucao', 'te_nat_instrucao',
       'te_nat_instrucao', 'te_nat_instrucao', 'te_nat_instrucao',
       'te_nat_instrucao', 'te_nat_instrucao', 'te_nat_instrucao',
       'te_nat_instrucao_idade', 'te_nat_instrucao_idade',
       'te_nat_instrucao_idade', 'te_nat_instrucao_idade', 'te_nat_raca',
       'te_nat_raca', 'te_nat_raca', 'te_nat_raca', 'te_nat_raca',
       'te_nat_raca', 'te_nat_raca_idade', 'te_nat_raca_idade',
       'te_nat_raca_idade', 'te_nat_raca_idade', 'te_nat_raca_idade',
       'te_nat_sexo', 'te_nat_sexo', 'te_ope', 'te_res', 'te_res_idade',
       'te_res_idade', 'te_res_idade', 'te_res_instrucao', 'te_res_instrucao',
       'te_res_instrucao', 'te_res_instrucao', 'te_res_instrucao',
       'te_res_instrucao', 'te_res_instrucao', 'te_res_instrucao',
       'te_res_instrucao', 'te_res_instrucao', 'te_res_instrucao',
       'te_res_instrucao_idade', 'te_res_instrucao_idade',
       'te_res_instrucao_idade', 'te_res_instrucao_idade', 'te_res_raca',
       'te_res_raca', 'te_res_raca', 'te_res_raca', 'te_res_raca',
       'te_res_raca', 'te_res_raca_idade', 'te_res_raca_idade',
       'te_res_raca_idade', 'te_res_raca_idade', 'te_res_raca_idade',
       'te_res_sexo', 'te_res_sexo', 'te_rgt', 'te_rgt_per_insp'],
      dtype='object', name='cd_indicador')
(753, 51)
1.0
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="3" halign="left">te_nat_idade</th>
      <th colspan="4" halign="left">te_nat_instrucao</th>
      <th>...</th>
      <th colspan="6" halign="left">te_res_raca</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>adult</th>
      <th>default</th>
      <th>minor</th>
      <th>5º Ano Completo</th>
      <th>6º ao 9º Ano Incompl</th>
      <th>&gt;Ignorado</th>
      <th>Analfabeto</th>
      <th>...</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Indígena ou Índia</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>10</td>
      <td>16</td>
      <td>27</td>
      <td>22</td>
      <td>0</td>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>11</td>
      <td>...</td>
      <td>63</td>
      <td>1</td>
      <td>0</td>
      <td>10</td>
      <td>1</td>
      <td>2</td>
      <td>5</td>
      <td>72</td>
      <td>203</td>
      <td>67.75</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>3</td>
      <td>6</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>16</td>
      <td>10.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>6</td>
      <td>2</td>
      <td>5</td>
      <td>0</td>
      <td>16</td>
      <td>17</td>
      <td>8.50</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>14</td>
      <td>14.00</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 51 columns</p>
</div>
<p>The final dataset has 51 features.</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler

<span class="hljs-comment"># Initialize a scaler, then apply it to the features</span>
scaler = MinMaxScaler() <span class="hljs-comment"># default=(0, 1)</span>
feats = df_final.axes[<span class="hljs-number">1</span>].get_level_values(<span class="hljs-number">0</span>)[:<span class="hljs-number">-1</span>]

df_final = pd.DataFrame(data = df_final)
df_final.loc[idx[:], idx[feats,:]] = scaler.fit_transform(df_final.loc[idx[:], idx[feats,:]])

df_final.head()
</div></code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="3" halign="left">te_nat_idade</th>
      <th colspan="4" halign="left">te_nat_instrucao</th>
      <th>...</th>
      <th colspan="6" halign="left">te_res_raca</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>adult</th>
      <th>default</th>
      <th>minor</th>
      <th>5º Ano Completo</th>
      <th>6º ao 9º Ano Incompl</th>
      <th>&gt;Ignorado</th>
      <th>Analfabeto</th>
      <th>...</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Indígena ou Índia</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>0.120000</td>
      <td>0.142857</td>
      <td>0.056250</td>
      <td>0.046025</td>
      <td>0.0</td>
      <td>0.357143</td>
      <td>0.0</td>
      <td>0.061224</td>
      <td>0.0</td>
      <td>0.026442</td>
      <td>...</td>
      <td>0.166227</td>
      <td>0.014085</td>
      <td>0.0</td>
      <td>0.089286</td>
      <td>0.023810</td>
      <td>0.027397</td>
      <td>0.04</td>
      <td>0.154176</td>
      <td>0.145115</td>
      <td>67.75</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.026667</td>
      <td>0.019048</td>
      <td>0.012500</td>
      <td>0.012552</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.020408</td>
      <td>0.0</td>
      <td>0.002404</td>
      <td>...</td>
      <td>0.010554</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.023810</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.010707</td>
      <td>0.010776</td>
      <td>10.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.004167</td>
      <td>0.004184</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.002639</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.002141</td>
      <td>0.000718</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.013333</td>
      <td>0.009524</td>
      <td>0.004167</td>
      <td>0.004184</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.002639</td>
      <td>0.028169</td>
      <td>0.0</td>
      <td>0.053571</td>
      <td>0.047619</td>
      <td>0.068493</td>
      <td>0.00</td>
      <td>0.034261</td>
      <td>0.011494</td>
      <td>8.50</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.000000</td>
      <td>0.009339</td>
      <td>14.00</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 51 columns</p>
</div>
<h3 id="setting-the-labeling-class">Setting the labeling class</h3>
<p>As proposed, the labeling class will be based on the rescues per inspection (te_rgt_per_insp). The labels (LOW, MEDIUM and HIGH) will be based on the terciles of its distribution.</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
sns.distplot(df_final.te_rgt_per_insp.default)

df_final.te_rgt_per_insp.default.describe()
</div></code></pre>
<pre><code>count    753.000000
mean      43.193323
std       72.725993
min        0.250000
25%        8.000000
50%       18.000000
75%       45.000000
max      836.500000
Name: default, dtype: float64
</code></pre>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_18_2.png" alt="png"></p>
<p>As the distribution shows, there's a concentration of rescues per inspection near the origin. It could mean few occurences of modern slavery, if there weren't municipalities that presented much higher rates - leading to a long tail in the distribution. Perfecting resources placement could lower inspections with near-zero rescues and bring the distribution to a normal display. At the same time, incresing effectiveness in enforcement may reduce the cases with the highest rates.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Categorization of the label feature</span>
df_with_labels = df_final
df_with_labels.loc[idx[:], idx[<span class="hljs-string">'te_rgt_per_insp'</span>,:]] = pd.qcut(
    df_final.te_rgt_per_insp.default,
    <span class="hljs-number">3</span>,
    labels=[<span class="hljs-string">"LOW"</span>, <span class="hljs-string">"MEDIUM"</span>, <span class="hljs-string">"HIGH"</span>]
)

print(df_with_labels.te_rgt_per_insp.default.describe())

print(<span class="hljs-string">'LOW MAX: '</span> + str(
    df_with_labels.loc[df_with_labels.te_rgt_per_insp.default == <span class="hljs-string">'LOW'</span>, idx[<span class="hljs-string">'te_rgt'</span>,:]].values.max())
)
print(<span class="hljs-string">'MEDIUM MAX: '</span> + str(
    df_with_labels.loc[df_with_labels.te_rgt_per_insp.default == <span class="hljs-string">'MEDIUM'</span>, idx[<span class="hljs-string">'te_rgt'</span>,:]].values.max())
)

df_with_labels.head()
</div></code></pre>
<pre><code>count     753
unique      3
top       LOW
freq      255
Name: default, dtype: object
LOW MAX: 0.04525862068965517
MEDIUM MAX: 0.13864942528735633
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>cd_indicador</th>
      <th>te_insp_rgt</th>
      <th>te_inspecoes</th>
      <th>te_nat</th>
      <th colspan="3" halign="left">te_nat_idade</th>
      <th colspan="4" halign="left">te_nat_instrucao</th>
      <th>...</th>
      <th colspan="6" halign="left">te_res_raca</th>
      <th colspan="2" halign="left">te_res_sexo</th>
      <th>te_rgt</th>
      <th>te_rgt_per_insp</th>
    </tr>
    <tr>
      <th>ds_agreg_primaria</th>
      <th>default</th>
      <th>default</th>
      <th>default</th>
      <th>adult</th>
      <th>default</th>
      <th>minor</th>
      <th>5º Ano Completo</th>
      <th>6º ao 9º Ano Incompl</th>
      <th>&gt;Ignorado</th>
      <th>Analfabeto</th>
      <th>...</th>
      <th>&gt;Não Informado</th>
      <th>Pessoa Que Se Enquadrar Como Branca</th>
      <th>Pessoa Que Se Enquadrar Como Indígena ou Índia</th>
      <th>Pessoa Que Se Enquadrar Como Parda ou Se Declarar Como Mulata, Cabocla, Cafuza, Mameluca ou Mestiça de Preto com Pessoa de Outra Cor ou Raça</th>
      <th>Pessoa Que Se Enquadrar Como Preta</th>
      <th>Pessoa Que Se Enquadrar Como de Raça Amarela ( de Origem Japonesa, Chinesa, Coreana, Etc)</th>
      <th>Feminino</th>
      <th>Masculino</th>
      <th>default</th>
      <th>default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>0.120000</td>
      <td>0.142857</td>
      <td>0.056250</td>
      <td>0.046025</td>
      <td>0.0</td>
      <td>0.357143</td>
      <td>0.0</td>
      <td>0.061224</td>
      <td>0.0</td>
      <td>0.026442</td>
      <td>...</td>
      <td>0.166227</td>
      <td>0.014085</td>
      <td>0.0</td>
      <td>0.089286</td>
      <td>0.023810</td>
      <td>0.027397</td>
      <td>0.04</td>
      <td>0.154176</td>
      <td>0.145115</td>
      <td>HIGH</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.026667</td>
      <td>0.019048</td>
      <td>0.012500</td>
      <td>0.012552</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.020408</td>
      <td>0.0</td>
      <td>0.002404</td>
      <td>...</td>
      <td>0.010554</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.023810</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.010707</td>
      <td>0.010776</td>
      <td>LOW</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.004167</td>
      <td>0.004184</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.002639</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.002141</td>
      <td>0.000718</td>
      <td>LOW</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.013333</td>
      <td>0.009524</td>
      <td>0.004167</td>
      <td>0.004184</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.002639</td>
      <td>0.028169</td>
      <td>0.0</td>
      <td>0.053571</td>
      <td>0.047619</td>
      <td>0.068493</td>
      <td>0.00</td>
      <td>0.034261</td>
      <td>0.011494</td>
      <td>LOW</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00</td>
      <td>0.000000</td>
      <td>0.009339</td>
      <td>MEDIUM</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 51 columns</p>
</div>
<h2 id="classification---random-forest">Classification - Random Forest</h2>
<h3 id="splitting-the-dataset">Splitting the dataset</h3>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

rs = <span class="hljs-number">64</span>

<span class="hljs-comment"># Split the features and labeling class in train, validate and test</span>
feats = df_final.axes[<span class="hljs-number">1</span>].get_level_values(<span class="hljs-number">0</span>)[:<span class="hljs-number">-1</span>]
labels = df_final.axes[<span class="hljs-number">1</span>].get_level_values(<span class="hljs-number">0</span>)[<span class="hljs-number">-1</span>]

feats_subset = df_final.loc[idx[:], idx[feats,:]]
labels_subset = df_final.loc[idx[:], idx[labels,:]]

<span class="hljs-comment"># Splitting in half for training/validation and testing</span>
X_trainvalidate, X_test, y_trainvalidate, y_test = train_test_split(
    feats_subset, labels_subset, test_size = <span class="hljs-number">0.5</span>, random_state = rs
)

<span class="hljs-comment"># Splitting the train/validate into training and validating subsets</span>
X_train, X_validate, y_train, y_validate = train_test_split(
    X_trainvalidate, y_trainvalidate, test_size = <span class="hljs-number">0.2</span>, random_state = rs
)

print(X_train.shape)
print(y_train.shape)
print(X_validate.shape)
print(y_validate.shape)
print(X_test.shape)
print(y_test.shape)
</div></code></pre>
<pre><code>(300, 50)
(300, 1)
(76, 50)
(76, 1)
(377, 50)
(377, 1)
</code></pre>
<h3 id="training-the-model">Training the model</h3>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_predict

classifier = RandomForestClassifier(n_estimators=<span class="hljs-number">10</span>, max_depth=<span class="hljs-number">10</span>, random_state=rs)
learner = classifier.fit(X_train, y_train.values.ravel())
predictions_train = cross_val_predict(learner, X_train, y_train.values.ravel(), cv=<span class="hljs-number">10</span>)

<span class="hljs-comment"># Metrics</span>
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> fbeta_score, accuracy_score

print(<span class="hljs-string">'Accuracy: '</span> + str(accuracy_score(y_train, predictions_train)))
print(<span class="hljs-string">'F Score (each label): '</span> + str(fbeta_score(y_train, predictions_train, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-keyword">None</span>)))
print(<span class="hljs-string">'F Score: '</span> + str(fbeta_score(y_train, predictions_train, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-string">'micro'</span>)))

<span class="hljs-comment"># Confusion matrix</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> scikitplot <span class="hljs-keyword">as</span> skplt

skplt.metrics.plot_confusion_matrix(y_train, predictions_train, normalize=<span class="hljs-keyword">False</span>)
skplt.metrics.plot_confusion_matrix(y_train, predictions_train, normalize=<span class="hljs-keyword">True</span>)
plt.show()
</div></code></pre>
<pre><code>Accuracy: 0.7833333333333333
F Score (each label): [0.78833693 0.83333333 0.73608618]
F Score: 0.7833333333333333
</code></pre>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_24_1.png" alt="png"></p>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_24_2.png" alt="png"></p>
<h3 id="validating-the-model">Validating the model</h3>
<pre class="hljs"><code><div>predictions_validate = cross_val_predict(learner, X_validate, y_validate.values.ravel(), cv=<span class="hljs-number">10</span>)

<span class="hljs-comment"># Metrics</span>
print(<span class="hljs-string">'Accuracy: '</span> + str(accuracy_score(y_validate, predictions_validate)))
print(<span class="hljs-string">'F Score (each label): '</span> + str(fbeta_score(y_validate, predictions_validate, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-keyword">None</span>)))
print(<span class="hljs-string">'F Score: '</span> + str(fbeta_score(y_validate, predictions_validate, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-string">'micro'</span>)))

<span class="hljs-comment"># Confusion matrix</span>
skplt.metrics.plot_confusion_matrix(y_validate, predictions_validate, normalize=<span class="hljs-keyword">False</span>)
skplt.metrics.plot_confusion_matrix(y_validate, predictions_validate, normalize=<span class="hljs-keyword">True</span>)
plt.show()
</div></code></pre>
<pre><code>Accuracy: 0.7631578947368421
F Score (each label): [0.7751938  0.76612903 0.7480315 ]
F Score: 0.7631578947368421
</code></pre>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_26_1.png" alt="png"></p>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_26_2.png" alt="png"></p>
<h3 id="testing-the-model">Testing the model</h3>
<pre class="hljs"><code><div>predictions_test = cross_val_predict(learner, X_test, y_test.values.ravel(), cv=<span class="hljs-number">10</span>)

<span class="hljs-comment"># Metrics</span>
print(<span class="hljs-string">'Accuracy: '</span> + str(accuracy_score(y_test, predictions_test)))
print(<span class="hljs-string">'F Score (each label): '</span> + str(fbeta_score(y_test, predictions_test, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-keyword">None</span>)))
print(<span class="hljs-string">'F Score: '</span> + str(fbeta_score(y_test, predictions_test, beta = <span class="hljs-number">0.5</span>, average=<span class="hljs-string">'micro'</span>)))

<span class="hljs-comment"># Confusion matrix</span>
skplt.metrics.plot_confusion_matrix(y_test, predictions_test, normalize=<span class="hljs-keyword">False</span>)
skplt.metrics.plot_confusion_matrix(y_test, predictions_test, normalize=<span class="hljs-keyword">True</span>)
plt.show()
</div></code></pre>
<pre><code>Accuracy: 0.8169761273209549
F Score (each label): [0.85149864 0.83710407 0.73770492]
F Score: 0.8169761273209549
</code></pre>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_28_1.png" alt="png"></p>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_28_2.png" alt="png"></p>
<h2 id="conclusion">Conclusion</h2>
<p>As expected, the larger the balanced subset, the more accurate the cross-validated prediction. The testing subset, that held 50% of the data had the best metrics overall (except for the MEDIUM label).</p>
<p>In the proposed real-world scenario, though, the accuracy can only be assessed by addressing resources to the flagged municipalities (labeled HIGH according to an expectation of maximizing rescues per inspection). It's very promising, nonetheless, to run such experimentations: in the normalized confusion matrix, for instance, we can see that 0.01 (2 occurences) of real HIGH were misclassified as LOW and 0.05 (7 instances) as MEDIUM, while 0.93 (125 municipalities) were correctly classified.</p>
<p>If law enforcement should address the 150 municipalities classified as HIGH, we should expect the same 125 places (~83.33%) to be correctly classified, therefore generating more rescues per inspection. 5 of them (~3.33%) would represent a low rate of success, since the municipalities labeled as LOW are near-zero rescues per inspections, from the distribution used for labeling (from 0 to ~0.045). Similarly, we'd have 20 inspections (~13.33% of the 150 labeled as HIGH) with fewer rescues (from ~0.045 to ~0.139) according to the ratings and the confusion matrix.</p>
<p>We can also verify the actual feature importance for the accurate predictions, witch may enable us to avoid rhetorical disputes and focus on tackling the problem at hand.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Display the most important features</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

importances = learner.feature_importances_

indices = np.argsort(importances)[::<span class="hljs-number">-1</span>]
columns = X_test.columns.values[indices[:<span class="hljs-number">10</span>]]
values = importances[indices][:<span class="hljs-number">10</span>]

<span class="hljs-comment"># Create the plot</span>
fig = plt.figure(figsize = (<span class="hljs-number">15</span>,<span class="hljs-number">10</span>))
plt.title(<span class="hljs-string">"Normalized Weights for First Ten Most Predictive Features"</span>, fontsize = <span class="hljs-number">16</span>)
plt.bar(np.arange(<span class="hljs-number">10</span>), values, width = <span class="hljs-number">0.6</span>, align=<span class="hljs-string">"center"</span>, color = <span class="hljs-string">'#00A000'</span>, \
      label = <span class="hljs-string">"Feature Weight"</span>)
plt.bar(np.arange(<span class="hljs-number">10</span>) - <span class="hljs-number">0.3</span>, np.cumsum(values), width = <span class="hljs-number">0.2</span>, align = <span class="hljs-string">"center"</span>, color = <span class="hljs-string">'#00A0A0'</span>, \
      label = <span class="hljs-string">"Cumulative Feature Weight"</span>)
plt.xticks(np.arange(<span class="hljs-number">10</span>), columns, rotation=<span class="hljs-string">'vertical'</span>)
plt.xlim((<span class="hljs-number">-0.5</span>, <span class="hljs-number">9.5</span>))
plt.ylabel(<span class="hljs-string">"Weight"</span>, fontsize = <span class="hljs-number">12</span>)
plt.xlabel(<span class="hljs-string">"Feature"</span>, fontsize = <span class="hljs-number">12</span>)

plt.legend(loc = <span class="hljs-string">'upper center'</span>)
plt.tight_layout()
plt.show()
</div></code></pre>
<p><img src="file:///Users/rmfagundes/Documents/Projetos/Estudos/udacity/mlengineer/uda_mlengineer/projects/capstone/report/output_30_0.png" alt="png"></p>
<p>Since the dataset used is closely related to inspections and rescues, it tends to a reinforced bias (the number of rescues - te_rgt -, for instance, is obviously extremely relevant to the classification. Some relevant features can bee seen, such as the ammount Male slaves rescued (te_res_sexo - Masculino), the low instruction level (te_res_instrucao and te_nat_instrucao - 5th and 6th to 9th year of formal education). Some issues show up also, such as the race not informed in the reports (te_nat_raca and te_res_raca) being among the most relevant features.</p>
<h3 id="future-work">Future work</h3>
<p>Since the results of this study is expected to be included in the Observatório Digital do Trabalho Escravo, the model will be built using the full inspections dataset. Then, it will be applied for labeling municipalities with no inspection record. In order for that to work, we must revisit the raw datasets from IBGE and the check if the high sparsity persists and, if so, how to address this issue.</p>
<p>Those municipalities, now identified, with a label HIGH will be flagged, as the ones already identified in the inspections records. The final output would be a brazilian map of municipalities colored by rating level, revealing which areas should be subject to a more thorough investigation.</p>

    </body>
    </html>